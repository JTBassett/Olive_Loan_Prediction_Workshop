{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Reading the dataset in a dataframe using Pandas\n",
    "df = _____\n",
    "\n",
    "#Let's look at the data!\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's learn about some of the numerical fields\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to understand the numerical data is to visualize it\n",
    "#We can plot one of these distributions easily in just one line\n",
    "df['ApplicantIncome'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also use box plots and segment the information out\n",
    "df.boxplot(column='ApplicantIncome', by = 'Education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To understand categorical (non-numerical) features we can look at the distribution to gain a better understanding \n",
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now let's get into part of what we are here for, predicting loan approval\n",
    "To me, one of the variables that could be important is whether you have a credit history\n",
    "We'll start by looking at the distribution of that field\n",
    "'''\n",
    "\n",
    "temp1 = df['Credit_History'].value_counts(ascending=True)\n",
    "temp2 = df.pivot_table(values='Loan_Status',index=['Credit_History'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())\n",
    "print ('Frequency Table for Credit History:') \n",
    "print (temp1)\n",
    "\n",
    "print ('\\nProbility of getting loan for each Credit History class:')\n",
    "print (temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As to be expected, it is much easier to get a loan once you have at least some credit history\n",
    "#This is ploted so it is easier to see below\n",
    "\n",
    "temp3 = _____\n",
    "temp3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also add gender into the mix and start creating a very basic classification algorithm \n",
    "#These are similar to pivot tables in excel, just with more functionality and can process more data\n",
    "temp4 = pd.crosstab([df.Credit_History, df.Gender], df['Loan_Status'])\n",
    "print(temp4)\n",
    "temp4.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now before we move along to build actual models, we must go back to some of the initial problems we noticed\n",
    "when we first reviewed the data. This included:\n",
    "1) Missing values\n",
    "2) Extremes/Outliers\n",
    "\n",
    "Topic (2) is sometthing we could look into given more time, but missing values are a much bigger issue\n",
    "and so I will only focus on (1) during this workshop\n",
    "'''\n",
    "\n",
    "#Use the function below to check the number of null (empty) values for each field\n",
    "df.apply(lambda x: sum(x.isnull()),axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "How might we address some of these errors?\n",
    "One way would be to get rid of that feature in our analysis entirely. \n",
    "- If this field is relevant, that may not be the best\n",
    "Another way would be to set it to the mean, median, or mode. We'll use the mean value below\n",
    "'''\n",
    "#Commented out because we will discuss a better way later\n",
    "#df['LoanAmount'].fillna(df['LoanAmount'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For categorical values it may be more difficult. In the case of binary variables we can choose the more common case\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Self_Employed'].fillna('No',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A more complicated way to fill in the null loan amount values would be to break find the mean/median/mode\n",
    "for each segment. In the code below, we break this up by the binary \"Self_Employed\" and \"Education\" features\n",
    "and then use the median value for each category, somewhat factoring out the extremes\n",
    "'''\n",
    "\n",
    "table = df.pivot_table(values='LoanAmount', index='Self_Employed' ,columns='Education', aggfunc=np.median)\n",
    "# Define function to return value of this pivot_table\n",
    "def fage(x):\n",
    " return table.loc[x['Self_Employed'],x['Education']]\n",
    "# Replace missing values\n",
    "df['LoanAmount'].fillna(df[df['LoanAmount'].isnull()].apply(fage, axis=1), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to factor in extremes is to use logs of the input values.\n",
    "#We see a much nicer distribution of Loan Amounts when we use that below\n",
    "\n",
    "df['LoanAmount_log'] = _____\n",
    "df['LoanAmount_log'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's finish out addressing the null values for the remaining fields:\n",
    "#Gender, Married, Dependents, Loan_Amount_Term, and Credit_History\n",
    "\n",
    "df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n",
    "df['Married'].fillna(df['Married'].mode()[0], inplace=True)\n",
    "df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\n",
    "df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "\n",
    "#Since credit history is such a deciding factor with many missing values, I'm going to drop these cases altogether\n",
    "df.dropna(axis=0, subset=['Credit_History'], inplace=True)\n",
    "\n",
    "#Clearly there are additional (probably better) ways to adjust dealing with these missing values, but given the\n",
    "#timeframe of the workshop we won't bother with dig into other options right now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that the null values are actually gone (it didn't work at first when I was going through this workshop)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can use the commented out code below to check the data types of each of the columns in your dataframe\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now before we actually test, make sure to seperate the labels from the features\n",
    "# Filter to only the features\n",
    "train_predictors = df.loc[:, df.columns != 'Loan_Status']\n",
    "# The target we're using to train the algorithm\n",
    "train_target = df['Loan_Status']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are so close to being able to build a model! \n",
    "#We'll begin the preparation by breaking the training data into training and validation data\n",
    "# (This is typically done when you have a medium-sized data set, and isn't as important here with only 624 samples)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_predictors, train_target, \n",
    "                                                                train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "#\"Cardinality\" means the number of unique values in a column - found in the nunique() function\n",
    "#Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "#Again, in our case this isn't as relevant but it would totally be for claims data\n",
    "categorical_cols = [cname for cname in X_train.columns if X_train[cname].nunique() < 10 and X_train[cname].dtype == \"object\"]\n",
    "\n",
    "#Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "#Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "train_df = X_train[my_cols].copy()\n",
    "valid_df = X_valid[my_cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll import a bunch of librarys we will use for a few steps of pre-processing, and then building models\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's pre-process the data to one-hot encode the categorical variables before building the model\n",
    "#Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "#Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "#Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can build our first model!!!\n",
    "\n",
    "#We'll use XGBoost classifier\n",
    "\n",
    "#Define the model\n",
    "XGB_model = ______\n",
    "\n",
    "#Bundle preprocessing and modeling code in a pipeline\n",
    "classifier = Pipeline(steps=[('preprocessor', _____),\n",
    "                      ('model', _____)\n",
    "                     ])\n",
    "\n",
    "#Preprocessing of training data, fit model \n",
    "classifier.fit(_____, _____)\n",
    "\n",
    "#Preprocessing of validation data, get predictions\n",
    "easypreds = classifier.predict(X_train)\n",
    "preds = classifier.predict(X_valid)\n",
    "\n",
    "accuracy = accuracy_score(y_train, easypreds)\n",
    "cv_accuracy = accuracy_score(y_valid, preds)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"Cross-Validation Accuracy: %.2f%%\" % (cv_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now that is probably not as great it as we'd like it to be, but what we can do is change the parameters\n",
    "\n",
    "The default parameters are given below:\n",
    "\n",
    "max_depth=3\n",
    "learning_rate=0.1\n",
    "n_estimators=100\n",
    "silent=True\n",
    "objective='binary:logistic'\n",
    "booster='gbtree'\n",
    "n_jobs=1\n",
    "nthread=None\n",
    "gamma=0\n",
    "min_child_weight=1\n",
    "max_delta_step=0\n",
    "subsample=1\n",
    "colsample_bytree=1\n",
    "colsample_bylevel=1\n",
    "reg_alpha=0\n",
    "reg_lambda=1\n",
    "scale_pos_weight=1\n",
    "base_score=0.5\n",
    "random_state=0\n",
    "seed=None\n",
    "missing=None\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We probably want a greater tree depth\n",
    "#Let's try it again below\n",
    "\n",
    "#Define model\n",
    "XGB_model2 = XGBClassifier(max_depth=_____)\n",
    "\n",
    "#Bundle preprocessing and modeling code in a pipeline\n",
    "classifier2 = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', XGB_model2)\n",
    "                     ])\n",
    "\n",
    "#Preprocessing of training data, fit model \n",
    "classifier2.fit(X_train, y_train)\n",
    "\n",
    "#Preprocessing of validation data, get predictions\n",
    "easypreds = classifier2.predict(X_train)\n",
    "preds = classifier2.predict(X_valid)\n",
    "\n",
    "accuracy = accuracy_score(y_train, easypreds)\n",
    "cv_accuracy = accuracy_score(y_valid, preds)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"Cross-Validation Accuracy: %.2f%%\" % (cv_accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
